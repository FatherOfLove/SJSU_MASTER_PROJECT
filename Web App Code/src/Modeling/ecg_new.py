# -*- coding: utf-8 -*-
"""ECG_new.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EljA2AtysvUIiLw5YwMypKjx36GJDoec
"""

#install wfdb library
!pip install wfdb

!pip install heartpy

from google.colab import drive
#drive.flush_and_unmount()
drive.mount('/gdrive')

#load library

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from io import StringIO
import json
from keras.utils import to_categorical
import tensorflow as tf
from sklearn.model_selection import train_test_split

#Model function
from keras.models import Sequential
from keras.layers import BatchNormalization
from keras.utils.np_utils import to_categorical # convert to one-hot-encoding
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D
from keras.optimizers import RMSprop, Adam
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ReduceLROnPlateau
from keras.constraints import max_norm
from sklearn.model_selection import train_test_split
import pywt
import datetime
import heartpy as hp

#load dataset directily from dat. and hea. files
import os
import wfdb
data_path='/gdrive/My Drive/Data set/ECG/ecg_dataset'
def read_data_files(data_path,label_list):
  path_names=os.listdir(data_path)
 # for name in path_names:
  #  if not os.path.isdir(name):
#      path_names.remove(name) 
  i=0
  records=[]      
             
  for patient_path in path_names:
      PATH=os.path.join(data_path,patient_path)   #path
      #if not os.path.isfile(PATH):
      FILES=os.listdir(PATH)
         #print(FILES) 
        # data_files=[]             
      for file in FILES:
        if '.hea' in file:
              #print(file)
              # data_files.append(file)
            
          record_path=os.path.join(PATH,file.split('.')[0])
          record=wfdb.rdrecord(record_path)
          
          label = record.comments[4].split(':')[1]

          if  label in label_list:
            print(label,file)  
            records.append(record)

  return records         

#all_records=read_data_files(data_path)
label_records = read_data_files(data_path,[' Myocardial infarction', ' Healthy control'])

# convert_label = {' Myocardial infarction':0, ' Healthy control':1}

# # denoise  function
# def denoise(data):
#     # wavedec
#     coeffs = pywt.wavedec(data=data, wavelet='db5', level=9)
#     cA9, cD9, cD8, cD7, cD6, cD5, cD4, cD3, cD2, cD1 = coeffs

#     # threshold
#     threshold = (np.median(np.abs(cD1)) / 0.6745) * (np.sqrt(2 * np.log(len(cD1))))
#     cD1.fill(0)
#     cD2.fill(0)
#     for i in range(1, len(coeffs) - 2):
#         coeffs[i] = pywt.threshold(coeffs[i], threshold)

#     rdata = pywt.waverec(coeffs=coeffs, wavelet='db5')
#     return rdata

# def wave_slicing(sig_data, pre_num, after_num, fs, slices_num):
#     data_length = slices_num *(pre_num + after_num)
#     if sig_data.shape[0] < data_length :
#       print('data is less than ', data_length )
#       return np.array()
#     sig_data = denoise(sig_data[0:data_length])
#     #print('sig_data shape',sig_data.shape)
#     #print(sig_data)
  

#     data_scale = hp.scale_data(sig_data)  
#     record_sliced = []
#     try:
#       # get the location of R wave
#       working_data, measures = hp.process(data_scale, fs)
#       # get index of R waves
#       peaklists = working_data['peaklist']
#       # remove the first one and last one
#       peaklists = peaklists[1:-1]
#       #print("index of R wavesï¼š", peaklists , '\n len(peaklists):', len(peaklists))

#       # get the wave   
#       for i in peaklists[0:slices_num]: 
#         #print(i)
#         tem_data = data_scale[i - pre_num: i + after_num] 
#         #print('sliced data shape', tem_data.shape)
#         #print('tem_data type:', type(tem_data))
#         record_sliced.append(tem_data)
#         #print('append tem_data')

#       #plot sample  
#       #plt.plot(record_sliced[-1]) 
#       #title = str(peaklists[-1]) 
#       #plt.title(title) 
#       #plt.show()

#       record_sliced = np.array(record_sliced).T

#       #print('record_sliced shape:',record_sliced.shape)
    
#     except Exception as e:
#       print('error:', e)

#     return record_sliced
  
# def make_dataset_waves(records, pre_num, after_num, fs, slices_num):
#   labels=[]
#   arrays_sigs=[]
#   for record in records:
#     #print(record.record_name)
#     label = record.comments[4].split(':')[1]
#     label_num = convert_label.get(label)
#     #print(label)
#     #print(label_num)
#     labels.append(label_num)
#     record_sliced_list = []
#     for sig_data in record.p_signal.T:
#       record_sliced = wave_slicing(sig_data, pre_num, after_num, fs, slices_num)
#       record_sliced_list.append(record_sliced)
    
#   arrays_sigs.append(np.array(record_sliced_list))
#   arrays_sigs = np.array(arrays_sigs)
#   labels = to_categorical(labels,2)
#   return arrays_sigs, labels

# X,y = make_dataset_waves(label_records, pre_num = 400, after_num = 600 , fs = 1000, slices_num = 30)
# X.shape

#do denoise hyperparameter here

def denoise(data):
    # wavedec
    coeffs = pywt.wavedec(data=data, wavelet='db8', level=9)
    cA9, cD9, cD8, cD7, cD6, cD5, cD4, cD3, cD2, cD1 = coeffs

    # threshold
    threshold = (np.median(np.abs(cD1)) / 0.6745) * (np.sqrt(2 * np.log(len(cD1))))
    cD1.fill(0)
    cD2.fill(0)
    for i in range(1, len(coeffs) - 2):
        coeffs[i] = pywt.threshold(coeffs[i], threshold)

    rdata = pywt.waverec(coeffs=coeffs, wavelet='db8')
    return rdata

convert_label = {' Myocardial infarction':0, ' Healthy control':1}
def make_dataset2(records): 
  labels=[]
  arrays_sigs=[]
  for record in records:
    #print(record.record_name)
    label = record.comments[4].split(':')[1]
    label_num = convert_label.get(label)
    #print(label)
    #print(label_num)
    labels.append(label_num)
    for i in range(0,14):
      record.p_signal.T[i] = denoise(record.p_signal.T[i])
    arrays_sigs.append(record.p_signal[100:23100])
  arrays_sigs = np.array(arrays_sigs)
  labels = to_categorical(labels,2)
  return arrays_sigs, labels

X , y = make_dataset2(label_records)

X.shape

y.shape

#split the train, test data from x,y
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)#try stratify
#split the valid data from train data
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)

y_test

def buildModel():

    newModel = tf.keras.models.Sequential([
                                           
        tf.keras.layers.InputLayer(input_shape=(23000,15)),

        tf.keras.layers.Conv1D(filters=4, kernel_size=21, strides=1, padding='SAME', activation='relu'),
       
        tf.keras.layers.AvgPool1D(pool_size=1, strides=4, padding='SAME'),
        
        tf.keras.layers.Conv1D(filters=16, kernel_size=23, strides=1, padding='SAME', activation='relu'),
        
        tf.keras.layers.AvgPool1D(pool_size=1, strides=4, padding='SAME'),
       
        tf.keras.layers.Conv1D(filters=32, kernel_size=25, strides=1, padding='SAME', activation='elu'),
       
        tf.keras.layers.AvgPool1D(pool_size=1, strides=8, padding='SAME'),
       
        tf.keras.layers.Conv1D(filters=64, kernel_size=27, strides=1, padding='SAME', activation='relu'),
       
        tf.keras.layers.Flatten(),
       
        tf.keras.layers.Dense(128, activation='relu'),
       
        tf.keras.layers.Dropout(rate=0.2),
       
        tf.keras.layers.Dense(2, activation='softmax')
    ])

    return newModel

model_test2 = buildModel()
model_test2.summary()

# Commented out IPython magic to ensure Python compatibility.
  #tbcb = keras.callbacks.TensorBoard(log_dir='./Graph', histogram_freq=0,  write_graph=True, write_images=True)
#   %load_ext tensorboard
  model_test2.compile(optimizer = Adam(lr=0.0000001, epsilon=1e-08, decay=0.0) , 
                  loss = "binary_crossentropy", metrics=["accuracy"])

  log_dir = "logs/fit_dec8/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)

  model_test2.fit(X_train, y_train, validation_data=(X_val, y_val),validation_batch_size = 4, epochs=1000,
                    batch_size=4 ,callbacks = [tensorboard_callback]
                  )

model_test2.evaluate(X_test,y_test,batch_size=1)

y_train

!tensorboard dev upload \
  --logdir /content/logs/ \
  --name " cnn db2,db5,db8" \
  --description "n_batch = 4,val_batch = 4, n_epochs = 1000,learning_rate=1e-7" \
  --one_shot

predictions = model_test2.predict(X_test)
labels = y_test

tf.math.confusion_matrix(
    labels, predictions, num_classes=2, weights=None, dtype=tf.dtypes.int16,
    name=None
)

predictions = tf.argmax(predictions,axis=1)

predictions

labels


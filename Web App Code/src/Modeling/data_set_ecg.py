# -*- coding: utf-8 -*-
"""20210205_ ECG_test_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KX2lCqqaKUyVYSI80aHrjZjzdmCMMzPd
"""

!pip install wfdb

"""## Download data"""

from google.colab import drive
drive.mount('/content/gdrive')

#!wget -r -N -c -np https://physionet.org/files/ptbdb/1.0.0/ /content/gdrive/

!ls

# Commented out IPython magic to ensure Python compatibility.
# %cd gdrive/

import os

data_path='/content/gdrive/My Drive/physionet.org/files/ptbdb/1.0.0'

import numpy as np

"""## discover the data"""

import wfdb
import numpy as np
import matplotlib.pyplot as plt
# plot ECG
def draw_ecg(x):
    plt.plot(x)
    plt.show()

#  read_ecg_data
def read_ecg_data(file_path,channel_names,sampfrom=0,smpto=1500):
    '''
    to read the ECG data
    sampfrom: start point，default sampfrom=0
    sampto：end poit，default to the end of data
    channel_names： chanel names which is a list，eg. channel_names=['vx']
    channels：a list with numbers，eg. channels=[0, 3]
    :return: record of ECG
    '''      
    record = wfdb.rdrecord(file_path, sampfrom=0, sampto = 1500 ,channel_names=['vz'])
    print(type(record))

    print(dir(record))
    
    print(record.p_signal)
    print(np.shape(record.p_signal))
    print(record.sig_len)
  
    print(record.record_name)
   
    print(record.n_sig)
   
    print(record.sig_name)
   
    print(record.fs)


    print("***************")
      

    
    print(wfdb.show_ann_labels())

    draw_ecg(record.p_signal)
    return record


#test function
test_file=data_path+'/patient001/s0014lre'
read_ecg_data(test_file,'vx')

"""# read all data to pd dataframe"""

import os
import wfdb
data_path='/content/gdrive/My Drive/physionet.org/files/ptbdb/1.0.0'

def read_data_files(data_path,label_list):
  path_names=os.listdir(data_path)
 # for name in path_names:
  #  if not os.path.isdir(name):
#      path_names.remove(name) 
  i=0
  records=[]      
             
  for patient_path in path_names:
      PATH=os.path.join(data_path,patient_path)   #path
      #if not os.path.isfile(PATH):
      FILES=os.listdir(PATH)
         #print(FILES) 
        # data_files=[]             
      for file in FILES:
        if '.hea' in file:
              #print(file)
              # data_files.append(file)
            
          record_path=os.path.join(PATH,file.split('.')[0])
          record=wfdb.rdrecord(record_path)
          
          label = record.comments[4].split(':')[1]

          if  label in label_list:
            print(label,file)  
            records.append(record)

  return records         

#all_records=read_data_files(data_path)
label_records = read_data_files(data_path,[' Myocardial infarction', ' Healthy control'])

len(label_records)

type(label_records[0])

print(type(label_records[0].p_signal))

print(label_records[0].p_signal.shape)

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from io import StringIO
import json
from keras.utils import to_categorical
import tensorflow as tf
from sklearn.model_selection import train_test_split

#Model function
from keras.models import Sequential
from keras.layers import BatchNormalization
from keras.utils.np_utils import to_categorical # convert to one-hot-encoding
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D
from keras.optimizers import RMSprop
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ReduceLROnPlateau
from keras.constraints import max_norm
from sklearn.model_selection import train_test_split
import pywt
import datetime

label_records[0].p_signal

"""# wave_slicing"""

#!pip install heartpy
#import heartpy as hp
import pandas as pd
import pywt
import matplotlib.pyplot as plt
convert_label = {' Myocardial infarction':0, ' Healthy control':1}

# denoise  function
def denoise(data):
    # wavedec
    coeffs = pywt.wavedec(data=data, wavelet='db5', level=9)
    cA9, cD9, cD8, cD7, cD6, cD5, cD4, cD3, cD2, cD1 = coeffs

    # threshold
    threshold = (np.median(np.abs(cD1)) / 0.6745) * (np.sqrt(2 * np.log(len(cD1))))
    cD1.fill(0)
    cD2.fill(0)
    for i in range(1, len(coeffs) - 2):
        coeffs[i] = pywt.threshold(coeffs[i], threshold)

    rdata = pywt.waverec(coeffs=coeffs, wavelet='db5')
    return rdata

def wave_slicing_lstm(sig_data, window_size):
    slice_size = window_size
    data_length = sig_data.shape[0]
    #sig_data = denoise(sig_data[0:data_length])
    #print('sig_data shape',sig_data.shape)
    #print('window_size',window_size)
    #print(sig_data)
    record_sliced = []
    try:
      n_windows = data_length//window_size -1 
      for i in range(n_windows): 
        tem_data = sig_data [i *slice_size : (i+1) *slice_size ]
        record_sliced.append(tem_data)
      #plot sample  
      #plt.plot(record_sliced[-1]) 
      #plt.show()

    except Exception as e:
      print('error:', e)

     #print(' record_sliced length:',len(record_sliced))   
    return record_sliced


def make_dataset_lstm(records, window_size):
  labels=[]
  record_sliced_list = []
  n_channels = len(records[0].sig_name)
  for record in records:
    #print(record.record_name)
    label = record.comments[4].split(':')[1]
    label_num = convert_label.get(label)    
    #print(label)
    #print(label_num)
    
    signals_array = record.p_signal.T
    is_first = True
    for i in range(len(signals_array)):
      sig_data = signals_array [i]
      #print(record.record_name+'_'+str(i)+' sig_data shape:', sig_data.shape)
      record_sliced = wave_slicing_lstm(sig_data, window_size)
      #print( record.record_name+'_'+str(i)+' record_sliced length:',len(record_sliced))
      for slice in record_sliced:
        record_sliced_list.append(slice)
        if is_first:   
          labels.append(label_num)
      is_first = False
  #print(len(record_sliced_list))
  record_sliced_list = np.array(record_sliced_list).reshape(-1, n_channels, window_size)
  labels = to_categorical(labels,2)
  return record_sliced_list, labels

X,y = make_dataset_lstm(label_records, window_size=1000)

#record0_sliced = wave_slicing_lstm(label_records[0].p_signal.T[0], window_size = 1000)

X.shape

draw_ecg(X[0][1])

y.shape

y[0]

#split the train, test data from x,y
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)#try stratify
#split the valid data from train data
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2

def make_model_lstm(input_shape, output_dim, lstm_layer, dropout=0.2):
    print("model dim: ", input_shape, output_dim)
    model = Sequential()
    model.add(lstm_layer(256, return_sequences=True, input_shape=input_shape, batch_size=None))
    model.add(Dropout(dropout))
    model.add(lstm_layer(128, return_sequences=True))
    model.add(Dropout(dropout))
    model.add(LSTM(64))
    model.add(Dropout(dropout))
    model.add(Dense(output_dim, activation='softmax'))
    model.compile(loss='categorical_crossentropy',metrics=[tf.keras.metrics.BinaryAccuracy()] , optimizer='adam')
    
    return model

class TimeHistory(tf.keras.callbacks.Callback):
    def on_train_begin(self, logs={}):
        self.times = []

    def on_epoch_begin(self, batch, logs={}):
        self.epoch_time_start = time.time()

    def on_epoch_end(self, batch, logs={}):
        self.times.append(time.time() - self.epoch_time_start)

# Commented out IPython magic to ensure Python compatibility.
#  %load_ext tensorboard
model_Path = '/gdrive/My Drive/model_output'

filepath = os.path.join(model_Path, "weights-improvement-{epoch:02d}-bigger.hdf5")
checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')

model_name = 'two_classes'
model_folder = os.path.join(model_Path,'tensorlogs/'+ model_name + "-logs/")

if not os.path.isdir(model_folder):
    n_logs = 0
else:
    n_logs = len(os.listdir(model_folder))
    
tensorboard_logs = os.path.join(model_folder, "%inth_run"%n_logs)
tensorboard_callback = keras.callbacks.TensorBoard(log_dir=tensorboard_logs, write_graph=False)
time_callback = TimeHistory()
callbacks = [checkpoint, tensorboard_callback, time_callback]

model = make_model((trainX.shape[1], trainX.shape[2]), trainY.shape[-1], LSTM)

model.fit(trainX, trainY, epochs=100, batch_size=256,sample_weight=weights, callbacks=callbacks)

model.evaluate(x= X_test, y = y_test, batch_size = 256)